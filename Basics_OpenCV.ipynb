{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "37oGgXsNR_Gq",
        "2UW9vEwQS9CP"
      ],
      "authorship_tag": "ABX9TyOBXP+R7cbzIXLSYwaExf70",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushs0911/OpenCV/blob/main/Basics_OpenCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmdJp9wRPdj0"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/images.zip\n",
        "!unzip -qq images.zip"
      ],
      "metadata": {
        "id": "U0ao_6YgR5wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Displaying Saving"
      ],
      "metadata": {
        "id": "37oGgXsNR_Gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"/content/images/castara.jpeg\")"
      ],
      "metadata": {
        "id": "21xpfc5GSkJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simple function to display images \n",
        "def imshow(title = \"\", image = None):\n",
        "  plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "68Jyn1mmR7of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imshow(\"Title\", image)"
      ],
      "metadata": {
        "id": "hFnqQGvzSaus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imwrite('output.jpg', image)"
      ],
      "metadata": {
        "id": "bXgENmJ6SqsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "id": "lbP58lmlSyER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grayscaling"
      ],
      "metadata": {
        "id": "2UW9vEwQS9CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our imshow function \n",
        "def imshow(title = \"Image\", image = None, size = 10):\n",
        "    w, h = image.shape[0], image.shape[1]\n",
        "    aspect_ratio = w/h\n",
        "    plt.figure(figsize=(size * aspect_ratio,size))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "uXbyTwdAS2-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flowers = cv2.imread(\"/content/images/flowers.jpeg\")"
      ],
      "metadata": {
        "id": "Fu40d42eUUkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imshow(\"Flowers\", flowers)"
      ],
      "metadata": {
        "id": "Bich-y5IT6SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gray_image = cv2.cvtColor(flowers, cv2.COLOR_BGR2GRAY)"
      ],
      "metadata": {
        "id": "JsRKAgSfUZnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imshow(\"COnverted to grayscale\", gray_image)"
      ],
      "metadata": {
        "id": "fh10fqTVVMf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grayscale just have 1 dimension, which is the intensity of gray. "
      ],
      "metadata": {
        "id": "EfwoW7SwVdm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gray_image.shape"
      ],
      "metadata": {
        "id": "fevH-QHjVUWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Color Spaces"
      ],
      "metadata": {
        "id": "XarKrT1LW0aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"/content/images/castara.jpeg\")"
      ],
      "metadata": {
        "id": "vRAU0CJWWMUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get each color space : `cv2.split`"
      ],
      "metadata": {
        "id": "bJnQ09dmXeBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, G, R = cv2.split(image)"
      ],
      "metadata": {
        "id": "4N1cP3y3W893"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B.shape, G.shape, R.shape"
      ],
      "metadata": {
        "id": "NFur53NTXtsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each color space on it's on will look like a grayscale as it lacks the other color channels\n"
      ],
      "metadata": {
        "id": "eduwxWziX4Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imshow(\"Blue channgel only\", B)"
      ],
      "metadata": {
        "id": "uV1Wj4aGXvd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Let's create a matrix of zeros \n",
        "# with dimensions of the image h x w  \n",
        "zeros = np.zeros(image.shape[:2], dtype = \"uint8\")\n",
        "imshow(\"Red\", cv2.merge([zeros, zeros, R]))\n",
        "imshow(\"Green\", cv2.merge([zeros, G, zeros]))\n",
        "imshow(\"Blue\", cv2.merge([B, zeros, zeros]))"
      ],
      "metadata": {
        "id": "L0RU4jgbX_O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged = cv2.merge([B, G, R])\n",
        "imshow(\"Merged\", merged)"
      ],
      "metadata": {
        "id": "i1SehunCYI_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can amplify individual colors "
      ],
      "metadata": {
        "id": "cPEkPRQPYaTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged = cv2.merge([B+100, G, R])\n",
        "imshow(\"Blue Boost\", merged)"
      ],
      "metadata": {
        "id": "fowQ8yF8YYIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HSV Color space"
      ],
      "metadata": {
        "id": "GFS8OS6mYo1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "imshow('HSV', hsv_image)"
      ],
      "metadata": {
        "id": "bybtCQBeYjZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This looks odd because our plotting function was designed for RGB only images, not HSV**"
      ],
      "metadata": {
        "id": "lg1w70sYZLT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB))"
      ],
      "metadata": {
        "id": "poc2hw9XY4Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switching back to viewing the RGB representation\n",
        "imshow(\"Hue\", hsv_image[:, :, 0])\n",
        "imshow(\"Saturation\", hsv_image[:, :, 1])\n",
        "imshow(\"Value\", hsv_image[:, :, 2])"
      ],
      "metadata": {
        "id": "vGtb_2T0ZhWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drawing on images"
      ],
      "metadata": {
        "id": "J9BoPH8xZ8Ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a black image using numpy"
      ],
      "metadata": {
        "id": "4QA9l_EjaNxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "black = np.zeros((512, 512, 3), np.uint8)\n",
        "imshow(\"BLACK CANVAS - RGB Color\", black)"
      ],
      "metadata": {
        "id": "rwn-CiTCZvX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#line on black square \n",
        "#this is an inplace operation, it changes the input image \n",
        "\n",
        "cv2.line(black, (0,0), (511,511), (225,127,0), 5)\n",
        "imshow(\"Diagnol Line\", black)"
      ],
      "metadata": {
        "id": "cGyxZMVnbOwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our black canvas again because now it has a line in it\n",
        "black = np.zeros((512,512,3), np.uint8)\n",
        "\n",
        "# Thickness - if positive. Negative thickness means that it is filled\n",
        "cv2.rectangle(black, (100,100), (300,250), (127,50,127), 10)\n",
        "imshow(\"Black Canvas With Pink Rectangle\", black)"
      ],
      "metadata": {
        "id": "wSciTuScbl7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```cv2.cirlce(image, center, radius, color, fill)```"
      ],
      "metadata": {
        "id": "jH9SMseGb5Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "black = np.zeros((512,512,3), np.uint8)\n",
        "\n",
        "cv2.circle(black, (350, 350), 100, (15,150,50), -1) \n",
        "imshow(\"Black Canvas With Green Circle\", black)"
      ],
      "metadata": {
        "id": "4xjvdm57bvN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```cv2.polylines(image, points, Closed?, color, thickness)```\n",
        "\n",
        "if Closed = True, we join the first and last points."
      ],
      "metadata": {
        "id": "M509mZtIb8In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = np.zeros((512,512,3), np.uint8)\n",
        "\n",
        "# Let's define four points\n",
        "pts = np.array( [[10,50], [400,50], [90,200], [50,500]], np.int32)\n",
        "\n",
        "# Let's now reshape our points in form  required by polylines\n",
        "pts = pts.reshape((-1,1,2))\n",
        "\n",
        "cv2.polylines(image, [pts], True, (0,0,255), 3)\n",
        "imshow(\"Black Canvas with Red Polygon\", image)"
      ],
      "metadata": {
        "id": "DK8JDk6Fb0pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding text\n",
        "```cv2.putText(image, 'Text to Display', bottom left starting point, Font, Font Size, Color, Thickness)```\n",
        "\n",
        "**Available Fonts**\n",
        "- FONT_HERSHEY_SIMPLEX, FONT_HERSHEY_PLAIN\n",
        "- FONT_HERSHEY_DUPLEX,FONT_HERSHEY_COMPLEX \n",
        "- FONT_HERSHEY_TRIPLEX, FONT_HERSHEY_COMPLEX_SMALL\n",
        "- FONT_HERSHEY_SCRIPT_SIMPLEX\n",
        "- FONT_HERSHEY_SCRIPT_COMPLEX"
      ],
      "metadata": {
        "id": "T5ItptMzcOjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = np.zeros((1000,1000,3), np.uint8)\n",
        "ourString =  'Hello This is Ayush !'\n",
        "cv2.putText(image, ourString, (155,290), cv2.FONT_HERSHEY_SCRIPT_COMPLEX, 3, (140,200,0), 2)\n",
        "imshow(\"Text on Image\", image)"
      ],
      "metadata": {
        "id": "WMQEgywQcGxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Translations and Rotations "
      ],
      "metadata": {
        "id": "RFDp8p0vdaz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using \n",
        "```\n",
        "cv2.warpAffine(image, T, (width, height))\n",
        "```\n",
        "to implement these transformations. "
      ],
      "metadata": {
        "id": "ONQIXPpCd9Vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kohli = cv2.imread(\"/content/Kohli.png\")\n",
        "imshow(\"Original\", kohli)\n"
      ],
      "metadata": {
        "id": "X2Y2kV2vdC91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing height and width of the imahe \n",
        "height, width = image.shape[:2]\n",
        "\n",
        "#we shift it by quarter of height and width \n",
        "quarter_height, quarter_width = height/4, width/4"
      ],
      "metadata": {
        "id": "yKvdFtOtem2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation matrix \n",
        "T = np.float32([[1, 0, quarter_width], [0, 1, quarter_height]])"
      ],
      "metadata": {
        "id": "AYjrwjjJe_A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_translation = cv2.warpAffine(kohli, T, (width, height))\n",
        "imshow(\"TRanslated\", img_translation)"
      ],
      "metadata": {
        "id": "L_oMu1wgfNwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(T)"
      ],
      "metadata": {
        "id": "fNyEWiwhiI6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(height, width)"
      ],
      "metadata": {
        "id": "bPeJQEcKiYKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using \n",
        "```\n",
        "cv2.getRotationMatrix2D( \n",
        "rotation_center_x, \n",
        "rotation_center_y, \n",
        "angle of rotation, \n",
        "scale\n",
        ")\n",
        "```\n",
        "For **Rotations**. "
      ],
      "metadata": {
        "id": "Av2kh7M1iy9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#divide by 2 to rotate the image around its center\n",
        "rotation_matrix = cv2.getRotationMatrix2D((width/2, height/2), 90, 1)"
      ],
      "metadata": {
        "id": "otf2JfXdibmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotated_image = cv2.warpAffine(kohli, rotation_matrix, (width, height))\n",
        "imshow(\"Rotated 90 degrees with scale = 1\", rotated_image)"
      ],
      "metadata": {
        "id": "dmrcWwhOjkOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotation_matrix = cv2.getRotationMatrix2D((width/2, height/2), 90, 0.5)\n",
        "print(rotation_matrix)\n",
        "# Input our image, the rotation matrix and our desired final width and height\n",
        "rotated_image = cv2.warpAffine(kohli, rotation_matrix, (width, height))\n",
        "imshow(\"Rotated 90 degrees with scale = 0.5\", rotated_image)"
      ],
      "metadata": {
        "id": "6Jf4jgLQjwb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `cv2.transpose()`"
      ],
      "metadata": {
        "id": "ayBTlZNdkCj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotated_image = cv2.transpose(kohli)\n",
        "imshow(\"Original\", kohli)\n",
        "imshow(\"Rotated using Transpose\", rotated_image)"
      ],
      "metadata": {
        "id": "hJCNn1uLj7gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now to a horizontal flip.\n",
        "flipped = cv2.flip(kohli, 1)\n",
        "imshow(\"Horizontal Flip\", flipped)"
      ],
      "metadata": {
        "id": "6bMQ4BtNkSTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scaling, Re-Sizing, Interpolations and Cropping. "
      ],
      "metadata": {
        "id": "8yiHMSGfkldA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Re-sizing** : changing the height and width <br>\n",
        "**Scaling** : changing the size, but keeping the width to height ratio constant, hence no distortions/skewing\n",
        "*********\n",
        "Resizing is executed using `cv2.resize()` function, \n",
        "```\n",
        "cv2.resize(image, \n",
        "          dsize(output image size),\n",
        "          x scale, \n",
        "          y scale, \n",
        "          interpolation)\n",
        "```\n",
        "**List of interpolation methods**\n",
        "\n",
        "- cv2.INTER_AREA - Good for shrinking or down sampling\n",
        "- cv2.INTER_NEAREST - Fastest\n",
        "- cv2.INTER_LINEAR - Good for zooming or up sampling (default)\n",
        "- cv2.INTER_CUBIC - Better\n",
        "- cv2.INTER_LANCZOS4 - Best"
      ],
      "metadata": {
        "id": "GU08-91pk3jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"/content/images/oxfordlibrary.jpeg\")\n",
        "imshow(\"Scaling -  Linear interpolation\", image)"
      ],
      "metadata": {
        "id": "gKxCjC4UkdES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if no interpolation is specified cv.INTER_LINEAR is used as defaukt \n",
        "# let's make our image 3/4 of its original size \n",
        "image_scaled = cv2.resize(image, None, fx = 0.75, fy =0.75)\n",
        "imshow(\"0.75x Scaling - Linear Interpolation\", image_scaled)"
      ],
      "metadata": {
        "id": "gpR9sVbBl66j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_scaled.shape"
      ],
      "metadata": {
        "id": "DFgzrtk_memk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's double the size of our image\n",
        "img_scaled2 = cv2.resize(image, None, fx=2, fy=2, interpolation = cv2.INTER_CUBIC)\n",
        "imshow(\"2x Scaling - Inter Cubic\", img_scaled2)"
      ],
      "metadata": {
        "id": "0_5dVmtNmjET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's double the size of our image using inter_nearest interpolation\n",
        "img_scaled3 = cv2.resize(image, None, fx=2, fy=2, interpolation = cv2.INTER_NEAREST)\n",
        "imshow(\"2x Scaling - Inter Nearest\", img_scaled3)"
      ],
      "metadata": {
        "id": "5_n6VFX4ms2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's skew the re-sizing by setting exact dimensions\n",
        "img_scaled4 = cv2.resize(image, (900, 400), interpolation = cv2.INTER_AREA)\n",
        "imshow(\"Scaling - Inter Area\", img_scaled4)"
      ],
      "metadata": {
        "id": "k9Vn0BpvmxEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Pyramids"
      ],
      "metadata": {
        "id": "qRddxFQcnCcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smaller = cv2.pyrDown(image)\n",
        "larger = cv2.pyrUp(smaller)\n",
        "\n",
        "imshow(\"Original\", image)\n",
        "imshow('Smaller', smaller)\n",
        "imshow('Larger', larger)\n",
        "\n",
        "even_smaller = cv2.pyrDown(smaller)\n",
        "imshow('Even Smaller', even_smaller)"
      ],
      "metadata": {
        "id": "RiODVoJXmzIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cropping"
      ],
      "metadata": {
        "id": "uo__nZ9CniF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get our image dimensions\n",
        "height, width = image.shape[:2]\n",
        "\n",
        "# Let's get the starting pixel coordiantes (top  left of cropping rectangle)\n",
        "# using 0.25 to get the x,y position that is 1/4 down from the top left (0,0)\n",
        "start_row, start_col = int(height * .25), int(width * .25)\n",
        "\n",
        "# Let's get the ending pixel coordinates (bottom right)\n",
        "end_row, end_col = int(height * .75), int(width * .75)\n",
        "\n",
        "# Simply use indexing to crop out the rectangle we desire\n",
        "cropped = image[start_row:end_row , start_col:end_col]\n",
        "\n",
        "imshow(\"Original Image\", image)\n",
        "\n",
        "# The cv2.rectangle function draws a rectangle over our image (in-place operation)\n",
        "copy = image.copy()\n",
        "cv2.rectangle(copy, (start_col,start_row), (end_col,end_row), (0,255,255), 10)\n",
        "\n",
        "imshow(\"Area we are cropping\", copy)\n",
        "\n",
        "imshow(\"Cropped Image\", cropped) "
      ],
      "metadata": {
        "id": "HBNVfUcTm9B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Airthmetic operations"
      ],
      "metadata": {
        "id": "KnK_mwS4oM_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Operations to directly add or subtract to color intensity. \n",
        "\n",
        "Calculates the per-element operation of 2 arrays. The overall effect is increasing or decreasing brightness. "
      ],
      "metadata": {
        "id": "2TM8bq6gpFS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"/content/images/liberty.jpeg\", 0)\n",
        "imshow(\"Grayscaled\", image)"
      ],
      "metadata": {
        "id": "ceUgf_tan8H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a matrix of ones, then multiply it by a scaler of 100\n",
        "# this gives a matrix with same dimensions of our image with all values being 100\n",
        "\n",
        "N = np.ones(image.shape, dtype = 'uint8')*100"
      ],
      "metadata": {
        "id": "GUyPdSUEpn_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N.shape, N"
      ],
      "metadata": {
        "id": "lS0J1_w6qFo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Increasing Brightness**"
      ],
      "metadata": {
        "id": "-yzojSL3qPrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "added = cv2.add(image, N)\n",
        "imshow(\"Increasing Brightness\", added)"
      ],
      "metadata": {
        "id": "rdBCSDZqqHDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simply adding it \n",
        "added2 = image + N\n",
        "imshow(\"Simple numpy adding results in clipping\", added2)"
      ],
      "metadata": {
        "id": "M_RrSsGuqePW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decreasing Brightness**"
      ],
      "metadata": {
        "id": "Gflh60fhqrD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Likewise we can also subtract\n",
        "# Notice the decrease in brightness\n",
        "subtracted = cv2.subtract(image, N)\n",
        "imshow(\"Subtracted\", subtracted)\n",
        "\n",
        "subtracted = image - N\n",
        "imshow(\"Subtracted 2\", subtracted)"
      ],
      "metadata": {
        "id": "zfg-QwF3qn52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bitwise operations and masking"
      ],
      "metadata": {
        "id": "bK6iVrwErSU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#making a sqaure \n",
        "square = np.zeros((300,300), np.uint8)\n",
        "cv2.rectangle(square, (50,50), (250,250), 255, -2)\n",
        "imshow(\"square\", square)\n",
        "\n",
        "# Making a ellipse\n",
        "ellipse = np.zeros((300, 300), np.uint8)\n",
        "cv2.ellipse(ellipse, (150, 150), (150, 150), 30, 0, 180, 255, -1)\n",
        "imshow(\"ellipse\", ellipse)"
      ],
      "metadata": {
        "id": "1PwV4r7dqxQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experimenting with some bitwise operations such as AND, OR, XOR and NOT**"
      ],
      "metadata": {
        "id": "EDykTVxpsF10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows only where they intersect\n",
        "And = cv2.bitwise_and(square, ellipse)\n",
        "imshow(\"AND\", And)\n",
        "\n",
        "# Shows where either square or ellipse is \n",
        "bitwiseOr = cv2.bitwise_or(square, ellipse)\n",
        "imshow(\"bitwiseOr\", bitwiseOr)\n",
        "\n",
        "# Shows where either exist by itself\n",
        "bitwiseXor = cv2.bitwise_xor(square, ellipse)\n",
        "imshow(\"bitwiseXor\", bitwiseXor)\n",
        "\n",
        "# Shows everything that isn't part of the square\n",
        "bitwiseNot_sq = cv2.bitwise_not(square)\n",
        "imshow(\"bitwiseNot_sq\", bitwiseNot_sq)"
      ],
      "metadata": {
        "id": "_QP9-vm6r-2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutions, Blurring and Sharpening Images"
      ],
      "metadata": {
        "id": "RrFkpCrFsxnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Blurring using Convolutions**"
      ],
      "metadata": {
        "id": "YrNPV2i1tD7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imshow(\"Flowers\", flowers)"
      ],
      "metadata": {
        "id": "PzW7LxPysM5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our 3 x 3 kernel\n",
        "kernel_3x3 = np.ones((3, 3), np.float32) / 9\n",
        "\n",
        "# We use the cv2.fitler2D to conovlve the kernal with an image \n",
        "blurred = cv2.filter2D(flowers, -1, kernel_3x3)\n",
        "imshow('3x3 Kernel Blurring', blurred)\n",
        "\n",
        "# Creating our 7 x 7 kernel\n",
        "kernel_7x7 = np.ones((7, 7), np.float32) / 49\n",
        "\n",
        "blurred2 = cv2.filter2D(flowers, -1, kernel_7x7)\n",
        "imshow('7x7 Kernel Blurring', blurred2)"
      ],
      "metadata": {
        "id": "8e4XFrkMs8GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other commonly used blurring methods in open cv\n",
        "- Regular blurring \n",
        "- Gaussian blurring \n",
        "- Median Blurring "
      ],
      "metadata": {
        "id": "1RuRHxFtyWgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaging done by convolving the image with a normalized box filter. \n",
        "# This takes the pixels under the box and replaces the central element\n",
        "# Box size needs to odd and positive \n",
        "blur = cv2.blur(flowers, (5,5))\n",
        "imshow('Averaging', blur)\n",
        "\n",
        "# Instead of box filter, gaussian kernel\n",
        "Gaussian = cv2.GaussianBlur(flowers, (5,5), 0)\n",
        "imshow('Gaussian Blurring', Gaussian)\n",
        "\n",
        "# Takes median of all the pixels under kernel area and central \n",
        "# element is replaced with this median value\n",
        "median = cv2.medianBlur(flowers, 5)\n",
        "imshow('Median Blurring', median)"
      ],
      "metadata": {
        "id": "PkX7MFm2tanb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bilateral Filter**\n",
        "#### ```dst = cv.bilateralFilter(src, d, sigmaColor, sigmaSpace[, dst[, borderType]])```\n",
        "\n",
        "**Bilateral is very effective in noise removal while keeping edges sharp**\n",
        "\n",
        "- **src**\tSource 8-bit or floating-point, 1-channel or 3-channel image.\n",
        "- **dst**\tDestination image of the same size and type as src .\n",
        "- **d**\tDiameter of each pixel neighborhood that is used during filtering. If it is non-positive, it is computed from sigmaSpace.\n",
        "- **sigmaColor**\tFilter sigma in the color space. A larger value of the parameter means that farther colors within the pixel neighborhood (see sigmaSpace) will be mixed together, resulting in larger areas of semi-equal color.\n",
        "- **sigmaSpace**\tFilter sigma in the coordinate space. A larger value of the parameter means that farther pixels will influence each other as long as their colors are close enough (see sigmaColor ). When d>0, it specifies the neighborhood size regardless of sigmaSpace. Otherwise, d is proportional to sigmaSpace.\n",
        "- **borderType**\tborder mode used to extrapolate pixels outside of the image"
      ],
      "metadata": {
        "id": "MkV5JkyEzPxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bilateral = cv2.bilateralFilter(flowers, 9, 75, 75)\n",
        "imshow('Bilateral Blurring', bilateral)"
      ],
      "metadata": {
        "id": "3GesJjriy6fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Image De-noising - Non-Local Means Denoising**\n",
        "\n",
        "**There are 4 variations of Non-Local Means Denoising:**\n",
        "\n",
        "- cv2.fastNlMeansDenoising() - works with a single grayscale images\n",
        "- cv2.fastNlMeansDenoisingColored() - works with a color image.\n",
        "- cv2.fastNlMeansDenoisingMulti() - works with image sequence captured in short period of time (grayscale images)\n",
        "- cv2.fastNlMeansDenoisingColoredMulti() - same as above, but for color images.\n",
        "\n",
        "```fastNlMeansDenoisingColored(InputArray src, OutputArray dst, float h=3, float hColor=3, int templateWindowSize=7, int searchWindowSize=21 )¶```\n",
        "\n",
        "#### Parameters for fastNlMeansDenoisingColored:\t\n",
        "\n",
        "- **src** – Input 8-bit 3-channel image.\n",
        "- **dst** – Output image with the same size and type as src .\n",
        "templateWindowSize – Size in pixels of the template patch that is used to compute weights. Should be odd. Recommended value 7 pixels\n",
        "- **searchWindowSize** – Size in pixels of the window that is used to compute weighted average for given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater denoising time. Recommended value 21 pixels\n",
        "- **h** – Parameter regulating filter strength for luminance component. Bigger h value perfectly removes noise but also removes image details, smaller h value preserves details but also preserves some noise\n",
        "- **hColor** – The same as h but for color components. For most images value equals 10 will be enought to remove colored noise and do not distort colors"
      ],
      "metadata": {
        "id": "vg9v8Es50b4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/hilton.jpeg')\n",
        "imshow('Original', image)\n",
        "\n",
        "dst = cv2.fastNlMeansDenoisingColored(image, None, 6, 6, 7, 21)\n",
        "imshow('fastNlMeansDenoisingColored', dst)"
      ],
      "metadata": {
        "id": "pyKIuZMV0R3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sharpening Images"
      ],
      "metadata": {
        "id": "cJ4JIvK40ybK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading our image\n",
        "imshow('Original', image)\n",
        "# Create our shapening kernel, remember it must sum to one \n",
        "kernel_sharpening = np.array([[-1,-1,-1], \n",
        "                              [-1, 9,-1],\n",
        "                              [-1,-1,-1]])\n",
        "\n",
        "# applying the sharpening kernel to the image\n",
        "sharpened = cv2.filter2D(image, -1, kernel_sharpening)\n",
        "imshow('Sharpened Image', sharpened)"
      ],
      "metadata": {
        "id": "G0Ojf5o60s3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Thresholding"
      ],
      "metadata": {
        "id": "RVlcUQ2f1W_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Thresholding Methods** \n",
        "\n",
        "![](https://raw.githubusercontent.com/rajeevratan84/ModernComputerVision/main/Screenshot%202020-11-17%20at%2012.57.55%20am.png)\n",
        "![](https://raw.githubusercontent.com/rajeevratan84/ModernComputerVision/main/Screenshot%202020-11-17%20at%2012.58.09%20am.png)\n",
        "\n",
        "\n",
        "https://docs.opencv.org/master/d7/d4d/tutorial_py_thresholding.html\n"
      ],
      "metadata": {
        "id": "kXDELsgc2n_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our image as greyscale \n",
        "image = cv2.imread('/content/images/scan.jpg',0)\n",
        "imshow(\"Original\", image)\n",
        "\n",
        "# Values below 127 goes to 0 or black, everything above goes to 255 (white)\n",
        "ret,thresh1 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
        "imshow('1 Threshold Binary @ 127', thresh1)\n",
        "\n",
        "# Values below 127 go to 255 and values above 127 go to 0 (reverse of above)\n",
        "ret,thresh2 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "imshow('2 Threshold Binary Inverse @ 127', thresh2)\n",
        "\n",
        "# Values above 127 are truncated (held) at 127 (the 255 argument is unused)\n",
        "ret,thresh3 = cv2.threshold(image, 127, 255, cv2.THRESH_TRUNC)\n",
        "imshow('3 THRESH TRUNC @ 127', thresh3)\n",
        "\n",
        "# Values below 127 go to 0, above 127 are unchanged  \n",
        "ret,thresh4 = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO)\n",
        "imshow('4 THRESH TOZERO @ 127', thresh4)\n",
        "\n",
        "# Reverse of the above, below 127 is unchanged, above 127 goes to 0\n",
        "ret,thresh5 = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO_INV)\n",
        "imshow('5 THRESH TOZERO INV @ 127', thresh5)"
      ],
      "metadata": {
        "id": "eKaCuAOW07mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Adaptive Thresholding**\n",
        "\n",
        "1. ADAPTIVE_THRESH_MEAN_C\n",
        "2. THRESH_OTSU\n",
        "\n",
        "#### **cv2.adaptiveThreshold Parameters**\n",
        "\n",
        "``**cv2.adaptiveThreshold**(src, maxValue, adaptiveMethod, thresholdType, blockSize, C[, dst]) → dst``\n",
        "\n",
        "- **src** – Source 8-bit single-channel image.\n",
        "- **dst** – Destination image of the same size and the same type as src .\n",
        "- **maxValue** – Non-zero value assigned to the pixels for which the condition is satisfied. See the details below.\n",
        "- **adaptiveMethod** – Adaptive thresholding algorithm to use, ADAPTIVE_THRESH_MEAN_C or ADAPTIVE_THRESH_GAUSSIAN_C . See the details below.\n",
        "- **thresholdType** – Thresholding type that must be either THRESH_BINARY or THRESH_BINARY_INV .\n",
        "- **blockSize** – Size of a pixel neighborhood that is used to calculate a threshold value for the pixel: 3, 5, 7, and so on.\n",
        "- **C** – Constant subtracted from the mean or weighted mean. Normally, it is positive but may be zero or negative as well.\n"
      ],
      "metadata": {
        "id": "UYuBwUXi3mVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/scan.jpg',0)\n",
        "\n",
        "\n",
        "# Values below 127 goes to 0 (black, everything above goes to 255 (white)\n",
        "ret,thresh1 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
        "imshow('Threshold Binary', thresh1)\n",
        "\n",
        "# It's good practice to blur images as it removes noise\n",
        "#image = cv2.GaussianBlur(image, (3, 3), 0)\n",
        "\n",
        "# Using adaptiveThreshold\n",
        "thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 5) \n",
        "imshow(\"Adaptive Mean Thresholding\", thresh) \n",
        "\n",
        "_, th2 = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "imshow(\"Otsu's Thresholding\", th2) \n",
        "\n",
        "# Otsu's thresholding after Gaussian filtering\n",
        "blur = cv2.GaussianBlur(image, (5,5), 0)\n",
        "_, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "imshow(\"Guassian Otsu's Thresholding\", th3) "
      ],
      "metadata": {
        "id": "aDh9ledf3Pq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SkImage Threshold Local**\n",
        "\n",
        "\n",
        "``threshold_local(image, block_size, offset=10)``\n",
        "\n",
        "The threshold_local function, calculates thresholds in regions with a characteristic size ``block_size`` surrounding each pixel (i.e. local neighborhoods). Each threshold value is the weighted mean of the local neighborhood minus an ``offset`` value\n",
        "\n",
        "\n",
        "https://scikit-image.org/docs/stable/auto_examples/applications/plot_thresholding.html"
      ],
      "metadata": {
        "id": "mhkvB2pJ3-L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.filters import threshold_local\n",
        "image = cv2.imread('/content/images/scan.jpg')\n",
        "\n",
        "\n",
        "# We get the Value component from the HSV color space \n",
        "# then we apply adaptive thresholdingto \n",
        "V = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))[2]\n",
        "T = threshold_local(V, 25, offset=15, method=\"gaussian\")\n",
        "\n",
        "# Apply the threshold operation \n",
        "thresh = (V > T).astype(\"uint8\") * 255\n",
        "imshow(\"threshold_local\", thresh)"
      ],
      "metadata": {
        "id": "pclvxzSn3tkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dilation, Erosion and Edge Detection "
      ],
      "metadata": {
        "id": "12F05ick54LL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Dilation** – Adds pixels to the boundaries of objects in an image\n",
        "- **Erosion** – Removes pixels at the boundaries of objects in an image\n",
        "- **Opening** - Erosion followed by dilation\n",
        "- **Closing** - Dilation followed by erosion \n",
        "\n",
        "![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/Screenshot%202021-11-15%20at%205.19.08%20pm.png)"
      ],
      "metadata": {
        "id": "qWVz9KRv6Bo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/opencv_inv.png', 0)\n",
        "imshow('Original', image)\n",
        "\n",
        "# Let's define our kernel size\n",
        "kernel = np.ones((5,5), np.uint8)\n",
        "\n",
        "# Now we erode\n",
        "erosion = cv2.erode(image, kernel, iterations = 1)\n",
        "imshow('Erosion', erosion)\n",
        "\n",
        "# Dilate here\n",
        "dilation = cv2.dilate(image, kernel, iterations = 1)\n",
        "imshow('Dilation', dilation)\n",
        "\n",
        "# Opening - Good for removing noise\n",
        "opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
        "imshow('Opening',opening)\n",
        "\n",
        "# Closing - Good for removing noise\n",
        "closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
        "imshow('Closing',closing)"
      ],
      "metadata": {
        "id": "kwIp21EP5BJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Canny Edge Detection** \n",
        "![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/Screenshot%202021-11-15%20at%205.24.15%20pm.png)\n",
        "\n",
        "- The first argument is our input image.\n",
        "- The second and third arguments are our minVal and maxVal respectively. \n",
        "- The forth argument is aperture_size. It is the size of Sobel kernel used for find image gradients. By default it is 3. \n",
        "\n",
        "Edge detection needs a threshold to tell what difference/change should be counted as edge"
      ],
      "metadata": {
        "id": "v2Nw3M2P7W2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/londonxmas.jpeg',0)\n",
        "\n",
        "# Canny Edge Detection uses gradient values as thresholds\n",
        "# The first threshold gradient\n",
        "canny = cv2.Canny(image, 50, 120)\n",
        "imshow('Canny 1', canny)\n",
        "\n",
        "# Wide edge thresholds expect lots of edges\n",
        "canny = cv2.Canny(image, 10, 200)\n",
        "imshow('Canny Wide', canny)\n",
        "\n",
        "# Narrow threshold, expect less edges \n",
        "canny = cv2.Canny(image, 200, 240)\n",
        "imshow('Canny Narrow', canny)\n",
        "\n",
        "canny = cv2.Canny(image, 60, 110)\n",
        "imshow('Canny 4', canny)\n",
        "\n",
        "##  Then, we need to provide two values: threshold1 and threshold2. Any gradient value larger than threshold2\n",
        "# is considered to be an edge. Any value below threshold1 is considered not to be an edge. \n",
        "#Values in between threshold1 and threshold2 are either classiﬁed as edges or non-edges based on how their \n",
        "#intensities are “connected”. In this case, any gradient values below 60 are considered non-edges\n",
        "#whereas any values above 120 are considered edges."
      ],
      "metadata": {
        "id": "J_auQUwx7KW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Auto canny"
      ],
      "metadata": {
        "id": "0B7SScy67zHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def autoCanny(image):\n",
        "  # Finds optimal thresholds based on median image pixel intensity\n",
        "  blurred_img = cv2.blur(image, ksize=(5,5))\n",
        "  med_val = np.median(image) \n",
        "  lower = int(max(0, 0.66 * med_val))\n",
        "  upper = int(min(255, 1.33 * med_val))\n",
        "  edges = cv2.Canny(image=image, threshold1=lower, threshold2=upper)\n",
        "  return edges\n",
        "\n",
        "auto_canny = autoCanny(image)\n",
        "imshow(\"auto canny\", auto_canny)"
      ],
      "metadata": {
        "id": "StWg8yzK7psm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}