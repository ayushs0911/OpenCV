{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNowu7MU4m6AgaDLCfofpB+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushs0911/OpenCV/blob/main/Basics_Image_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJe6SotIUv9X"
      },
      "outputs": [],
      "source": [
        "import cv2 \n",
        "import numpy as np \n",
        "from matplotlib import pyplot as plt \n",
        "\n",
        "def imshow(title = \"Image\", image = None, size =10):\n",
        "  w, h = image.shape[0], image.shape[1]\n",
        "  aspect_ratio = w/h\n",
        "  plt.figure(figsize = (size*aspect_ratio, size))\n",
        "  plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/images.zip\n",
        "!unzip -qq images.zip"
      ],
      "metadata": {
        "id": "plCbT0LzXGUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contours \n",
        "These are the continous lines or curves that bound or cover the full boundary of an object. "
      ],
      "metadata": {
        "id": "mkmCYA9tXLKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"/content/images/LP.jpg\")\n",
        "imshow(\"Input\", image)"
      ],
      "metadata": {
        "id": "OB1fOQcGXJKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying `cv2.findContours()`"
      ],
      "metadata": {
        "id": "HRXIzkXJX1nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```cv2.findContours(image, Retrieval Mode, Approximation Method)```\n",
        "\n",
        "**Retrieval Modes**\n",
        "- **RETR_LIST** - Retrieves all the contours, but doesn't create any parent-child relationship. Parents and kids are equal under this rule, and they are just contours. ie they all belongs to same hierarchy level.\n",
        "- **RETR_EXTERNAL** - returns only extreme outer flags. All child contours are left behind.\n",
        "- **RETR_CCOMP** - This flag retrieves all the contours and arranges them to a 2-level hierarchy. ie external contours of the object (ie its boundary) are placed in hierarchy-1. And the contours of holes inside object (if any) is placed in hierarchy-2. If any object inside it, its contour is placed again in hierarchy-1 only. And its hole in hierarchy-2 and so on.\n",
        "- **RETR_TREE** -  It retrieves all the contours and creates a full family hierarchy list. \n",
        "\n",
        "**Approximation Method Options**\n",
        "- cv2.CHAIN_APPROX_NONE – Stores all the points along the line(inefficient!)\n",
        "- cv2.CHAIN_APPROX_SIMPLE – Stores the end points of each line\n"
      ],
      "metadata": {
        "id": "ZHsGk9_TX_ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#covert to grayscale \n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "_, th2 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "imshow(\"After thresholding\", th2)\n",
        "\n",
        "contours, hierarchy = cv2.findContours(th2, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "#draw all contours -> this overides the input image (inplace operation)\n",
        "# Use '-1' as the 3rd parameter to draw all\n",
        "cv2.drawContours(image, contours, -1, (0,255,0), thickness = 2)\n",
        "imshow('Contours overlaid on original image', image)\n",
        "\n",
        "print(\"Number of Contours found = \" + str(len(contours)))\n"
      ],
      "metadata": {
        "id": "0OSFPG3-XmtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contours[0]"
      ],
      "metadata": {
        "id": "SBW6ZlrTZ6gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### what if we don't apply threshold?"
      ],
      "metadata": {
        "id": "bUuJoMsqaKta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/LP.jpg')\n",
        "\n",
        "# Convert to Grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "imshow('After Grayscaling', gray)\n",
        "\n",
        "# Finding Contours\n",
        "contours, hierarchy = cv2.findContours(gray, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "# Draw all contours, note this overwrites the input image (inplace operation)\n",
        "# Use '-1' as the 3rd parameter to draw all\n",
        "#cv2.drawContours(image, contours, -1, (0,255,0), thickness = 2)\n",
        "imshow('Contours overlaid on original image', image)\n",
        "\n",
        "print(\"Number of Contours found = \" + str(len(contours)))"
      ],
      "metadata": {
        "id": "xW3Kx9JYaFYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE: For findContours to work, the background has to be black and foreground (i.e. the text or objects)** \n",
        "<br>Otherwise you'll need to invert the image by using **cv2..bitwise_not(input_image)**\n",
        "\n",
        "#### We can use canny edges instead of thresholding"
      ],
      "metadata": {
        "id": "mtNjhoRxafxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/LP.jpg')\n",
        "\n",
        "# Convert to Grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Canny Edges\n",
        "edged = cv2.Canny(gray, 30, 200)\n",
        "imshow('Canny Edges', edged)\n",
        "\n",
        "# Finding Contours\n",
        "contours, hierarchy = cv2.findContours(edged, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "cv2.drawContours(image, contours, -1, (0,255,0), thickness = 2)\n",
        "imshow('Contours overlaid on original image', image)\n",
        "\n",
        "print(\"Number of Contours found = \" + str(len(contours)))\n"
      ],
      "metadata": {
        "id": "BCg1Le-VaZj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** : Blurring before Step2 is recommended to remove noisy contours. "
      ],
      "metadata": {
        "id": "OluTVZMYa8P_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retreival Modes \n",
        "Official Doc - https://docs.opencv.org/master/d9/d8b/tutorial_py_contours_hierarchy.html\n",
        "\n",
        "**Hierachry**\n",
        "\n",
        "This array stores 4 values for each contour:\n",
        "- First term is the index of the Next contour\n",
        "- Second term is the index of the Previous contour\n",
        "- Third term is the index of the parent contour\n",
        "- Forth term is the index of the child contour"
      ],
      "metadata": {
        "id": "TXvZsFg6bIA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RETR_LIST**\n",
        "Retrieves all the contours, but doesn't create any parent-child relationship. Parents and kids are equal under this rule, and they are just contours. ie they all belongs to same hierarchy level."
      ],
      "metadata": {
        "id": "tYKkEglLbTI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/LP.jpg')\n",
        "\n",
        "# Convert to Grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "_, th2 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "imshow('After thresholding', th2)\n",
        "\n",
        "# Use a copy of your image e.g. edged.copy(), since findContours alters the image\n",
        "contours, hierarchy = cv2.findContours(th2, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "# Draw all contours, note this overwrites the input image (inplace operation)\n",
        "# Use '-1' as the 3rd parameter to draw all\n",
        "cv2.drawContours(image, contours, -1, (0,255,0), thickness = 2)\n",
        "imshow('Contours overlaid on original image', image)\n",
        "\n",
        "print(\"Number of Contours found = \" + str(len(contours)))\n",
        "print(hierarchy)"
      ],
      "metadata": {
        "id": "wLYdmwAoatPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RETR_EXTERNAL**\n",
        "\n",
        "Returns only extreme outer flags. All child contours are left behind."
      ],
      "metadata": {
        "id": "vamCv-xSbmSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/LP.jpg')\n",
        "\n",
        "# Convert to Grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "_, th2 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "imshow('After thresholding', th2)\n",
        "\n",
        "# Use a copy of your image e.g. edged.copy(), since findContours alters the image\n",
        "contours, hierarchy = cv2.findContours(th2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "# Draw all contours, note this overwrites the input image (inplace operation)\n",
        "# Use '-1' as the 3rd parameter to draw all\n",
        "cv2.drawContours(image, contours, -1, (0,255,0), thickness = 2)\n",
        "imshow('Contours overlaid on original image', image, size = 10)\n",
        "\n",
        "print(\"Number of Contours found = \" + str(len(contours)))\n",
        "print(hierarchy)"
      ],
      "metadata": {
        "id": "zpPcXugWbf-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RETR_CCOMP**\n",
        "\n",
        "Retrieves all the contours and arranges them to a 2-level hierarchy. ie external contours of the object (ie its boundary) are placed in hierarchy-1. And the contours of holes inside object (if any) is placed in hierarchy-2. If any object inside it, its contour is placed again in hierarchy-1 only. And its hole in hierarchy-2 and so on."
      ],
      "metadata": {
        "id": "8270-gVSb2Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/LP.jpg')\n",
        "\n",
        "# Convert to Grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "_, th2 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "imshow('After thresholding', th2)\n",
        "\n",
        "# Use a copy of your image e.g. edged.copy(), since findContours alters the image\n",
        "contours, hierarchy = cv2.findContours(th2, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "# Draw all contours, note this overwrites the input image (inplace operation)\n",
        "# Use '-1' as the 3rd parameter to draw all\n",
        "cv2.drawContours(image, contours, -1, (0,255,0), thickness = 2)\n",
        "imshow('Contours overlaid on original image', image)\n",
        "\n",
        "print(\"Number of Contours found = \" + str(len(contours)))\n",
        "print(hierarchy)"
      ],
      "metadata": {
        "id": "-a7OLJjZbpwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RETR_TREE**\n",
        "It retrieves all the contours and creates a full family hierarchy list."
      ],
      "metadata": {
        "id": "oT8bCleDb4dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/LP.jpg')\n",
        "\n",
        "# Convert to Grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "_, th2 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "imshow('After thresholding', th2)\n",
        "\n",
        "# Use a copy of your image e.g. edged.copy(), since findContours alters the image\n",
        "contours, hierarchy = cv2.findContours(th2, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "# Draw all contours, note this overwrites the input image (inplace operation)\n",
        "# Use '-1' as the 3rd parameter to draw all\n",
        "cv2.drawContours(image, contours, -1, (0,255,0), thickness = 2)\n",
        "imshow('Contours overlaid on original image', image)\n",
        "\n",
        "print(\"Number of Contours found = \" + str(len(contours)))\n",
        "print(hierarchy)"
      ],
      "metadata": {
        "id": "sGIkpoNgb63F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Contouring Modes**\n",
        "\n",
        "#### **CHAIN_APPROX_NONE**"
      ],
      "metadata": {
        "id": "rC1hai42b8l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/LP.jpg')\n",
        "\n",
        "# Convert to Grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "_, th2 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "imshow('After thresholding', th2)\n",
        "\n",
        "# Use a copy of your image e.g. edged.copy(), since findContours alters the image\n",
        "contours, hierarchy = cv2.findContours(th2, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "# Draw all contours, note this overwrites the input image (inplace operation)\n",
        "# Use '-1' as the 3rd parameter to draw all\n",
        "cv2.drawContours(image, contours, -1, (0,255,0), thickness = 2)\n",
        "imshow('Contours overlaid on original image', image)\n",
        "\n",
        "print(\"Number of Contours found = \" + str(len(contours)))\n",
        "for c in contours:\n",
        "  print(len(c))"
      ],
      "metadata": {
        "id": "JroMnEYVb-tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **CHAIN_APPROX_SIMPLE**"
      ],
      "metadata": {
        "id": "ycxkVQ9vcAJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/images/LP.jpg')\n",
        "\n",
        "# Convert to Grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "_, th2 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "imshow('After thresholding', th2)\n",
        "\n",
        "# Use a copy of your image e.g. edged.copy(), since findContours alters the image\n",
        "contours, hierarchy = cv2.findContours(th2, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Draw all contours, note this overwrites the input image (inplace operation)\n",
        "# Use '-1' as the 3rd parameter to draw all\n",
        "cv2.drawContours(image, contours, -1, (0,255,0), thickness = 2)\n",
        "imshow('Contours overlaid on original image', image)\n",
        "\n",
        "print(\"Number of Contours found = \" + str(len(contours)))\n",
        "for c in contours:\n",
        "  print(len(c))"
      ],
      "metadata": {
        "id": "JHEV1a6BcB74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Moments, Sorting, Approximating & Matching Contours**"
      ],
      "metadata": {
        "id": "Tgu8_h5wchBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding Contours**"
      ],
      "metadata": {
        "id": "FoQa-wt0clxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our image\n",
        "image = cv2.imread('/content/images/bunchofshapes.jpg')\n",
        "imshow('Original Image', image)\n",
        "\n",
        "# Grayscale our image\n",
        "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Find Canny edges\n",
        "edged = cv2.Canny(gray, 50, 200)\n",
        "imshow('Canny Edges', edged)\n",
        "\n",
        "# Find contours and print how many were found\n",
        "contours, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "print(\"Number of contours found = \", len(contours))\n",
        "\n",
        "# Draw all contours over blank image\n",
        "cv2.drawContours(image, contours, -1, (0,255,0), 3)\n",
        "imshow('All Contours', image)"
      ],
      "metadata": {
        "id": "An7IWtE4cMGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sorting by Area using cv2.ContourArea and cv2.Moments**\n",
        "\n",
        "![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/moments.png)"
      ],
      "metadata": {
        "id": "1bfhSuX3c-iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function we'll use to display contour area\n",
        "\n",
        "def get_contour_areas(contours):\n",
        "    \"\"\"returns the areas of all contours as list\"\"\"\n",
        "    all_areas = []\n",
        "    for cnt in contours:\n",
        "        area = cv2.contourArea(cnt)\n",
        "        all_areas.append(area)\n",
        "    return all_areas\n",
        "\n",
        "# Load our image\n",
        "image = cv2.imread('/content/images/bunchofshapes.jpg')\n",
        "\n",
        "# Let's print the areas of the contours before sorting\n",
        "print(\"Contor Areas before sorting...\")\n",
        "print(get_contour_areas(contours))\n",
        "\n",
        "# Sort contours large to small by area\n",
        "sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
        "\n",
        "print(\"Contor Areas after sorting...\") \n",
        "print(get_contour_areas(sorted_contours))\n",
        "\n",
        "# Iterate over our contours and draw one at a time\n",
        "for (i,c) in enumerate(sorted_contours):\n",
        "    M = cv2.moments(c)\n",
        "    cx = int(M['m10'] / M['m00'])\n",
        "    cy = int(M['m01'] / M['m00'])\n",
        "    cv2.putText(image, str(i+1), (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
        "    cv2.drawContours(image, [c], -1, (255,0,0), 3)\n",
        "\n",
        "imshow('Contours by area', image)"
      ],
      "metadata": {
        "id": "mtxSgZn2ctUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for sorting by position "
      ],
      "metadata": {
        "id": "SWOjgD6GubCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions we'll use for sorting by position\n",
        "def x_cord_contour(contours):\n",
        "    \"\"\"Returns the X cordinate for the contour centroid\"\"\"\n",
        "    if cv2.contourArea(contours) > 10:\n",
        "        M = cv2.moments(contours)\n",
        "        return (int(M['m10']/M['m00']))\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "def label_contour_center(image, c):\n",
        "    \"\"\"Places a red circle on the centers of contours\"\"\"\n",
        "    M = cv2.moments(c)\n",
        "    cx = int(M['m10'] / M['m00'])\n",
        "    cy = int(M['m01'] / M['m00'])\n",
        "    \n",
        "    # Draw the countour number on the image\n",
        "    cv2.circle(image,(cx,cy), 10, (0,0,255), -1)\n",
        "    return image"
      ],
      "metadata": {
        "id": "VCD6qLf-ubti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use moments to calculate the center and then use the X coordinate to csort from left to right. "
      ],
      "metadata": {
        "id": "qEZN_Kjp5spf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our image\n",
        "image = cv2.imread('images/bunchofshapes.jpg')\n",
        "orginal_image = image.copy()\n",
        "\n",
        "#compute center of mass or centroids and draw them on image \n",
        "for (i,c) in enumerate(contours):\n",
        "  orig = label_contour_center(image, c)\n",
        "\n",
        "#showing the contour centers \n",
        "imshow(\"Soorting left to right\", image)\n",
        "\n",
        "#sort by left to right using our x_cord_contour function \n",
        "contours_left_to_right = sorted(contours, key = x_cord_contour, reverse = False)\n",
        "\n",
        "# Labeling Contours left to right\n",
        "for (i,c)  in enumerate(contours_left_to_right):\n",
        "    cv2.drawContours(orginal_image, [c], -1, (0,0,255), 3)  \n",
        "    M = cv2.moments(c)\n",
        "    cx = int(M['m10'] / M['m00'])\n",
        "    cy = int(M['m01'] / M['m00'])\n",
        "    cv2.putText(orginal_image, str(i+1), (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
        "    (x, y, w, h) = cv2.boundingRect(c)  \n",
        "\n",
        "imshow('Sorting Left to Right', orginal_image)"
      ],
      "metadata": {
        "id": "-Q94jQ2iubq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Approximating Contours using ApproxPolyDP**\n",
        "\n",
        "### **Using ApproxPolyDP to approximate contours as a more defined shape**\n",
        "It approximates a contour shape to another shape with less number of vertices depending upon the precision we specify.\n",
        "\n",
        "\n",
        "***cv2.approxPolyDP(contour, Approximation Accuracy, Closed)***\n",
        "- **contour** – is the individual contour we wish to approximate\n",
        "- **Approximation Accuracy** – Important parameter is determining the accuracy of the approximation. Small values give precise-  approximations, large values give more generic approximation. A good rule of thumb is less than 5% of the contour perimeter\n",
        "- **Closed** – a Boolean value that states whether the approximate contour should be open or closed \n"
      ],
      "metadata": {
        "id": "FIQQcvwf7Lt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `cv2.boundingRect(contour)` is a method provided by the OpenCV library in Python. It calculates the bounding rectangle for a given contour, which is the smallest rectangle that completely encompasses the contour.\n",
        "\n",
        "When you pass a contour to `cv2.boundingRect()`, it returns four values: (x, y, width, height). Here's what each of these values represents:\n",
        "\n",
        "- `x`: The x-coordinate of the top-left corner of the bounding rectangle.\n",
        "- `y`: The y-coordinate of the top-left corner of the bounding rectangle.\n",
        "- `width`: The width of the bounding rectangle.\n",
        "- `height`: The height of the bounding rectangle.\n",
        "\n",
        "By using these values, you can obtain the coordinates and dimensions of the bounding rectangle, which can be useful for tasks such as object detection, tracking, or cropping."
      ],
      "metadata": {
        "id": "SPd2i1ol9-sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "image = cv2.imread(\"/content/images/house.jpg\")\n",
        "orig_image = image.copy()\n",
        "imshow(\"Original Image\", orig_image)\n",
        "\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "rest, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "#find contours \n",
        "contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "copy = image.copy()\n",
        "\n",
        "#iterate through each contour \n",
        "\n",
        "for c in contours:\n",
        "  x, y, w, h = cv2.boundingRect(c)\n",
        "  cv2.rectangle(orig_image, (x,y), (x+w, y+h), (0,0,255), 2)\n",
        "  cv2.drawContours(image, [c], 0, (0,255,0), 2)\n",
        "\n",
        "imshow('Drawing of contours', image)\n",
        "imshow('Bounding rectangles', orig_image)\n",
        "\n",
        "\n",
        "#iterate through each contour and compute the approx contour \n",
        "for c in contours:\n",
        "  #calculate accuracy as a percent of the contour perimeter \n",
        "  accuracy = 0.03*cv2.arcLength(c, True)\n",
        "  approx = cv2.approxPolyDP(c, accuracy, True)\n",
        "  cv2.drawContours(copy, [approx], 0, (0, 255,0), 2)\n",
        "\n",
        "imshow('Approx Poly DP', copy)\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "v5fOF8SKdglj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convex Hull \n",
        "\n",
        "![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/convex.png)\n",
        "\n",
        "Convex Hull will look similar to contour approximation, but it is not (Both may provide the same results in some cases). \n",
        "\n",
        "The cv2.convexHull() function checks a curve for convexity defects and corrects it. Generally speaking, convex curves are the curves which are always bulged out, or at-least flat. And if it is bulged inside, it is called convexity defects. For example, check the below image of hand. Red line shows the convex hull of hand. The double-sided arrow marks shows the convexity defects, which are the local maximum deviations of hull from contours"
      ],
      "metadata": {
        "id": "kTvmLkS9_uQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"/content/images/hand.jpg\")\n",
        "original_image = image.copy()\n",
        "\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "imshow(\"Original Image\", image)\n",
        "\n",
        "#threshold the image\n",
        "ret, thresh = cv2.threshold(gray, 176, 255, 0)\n",
        "\n",
        "#find contours \n",
        "contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "cv2.drawContours(image, contours, 0, (0, 255,0), 2)\n",
        "imshow('Contours of Hand', image)\n",
        "\n",
        "#sort contours by area and then remove the largest frame contour \n",
        "n = len(contours) - 1\n",
        "contours = sorted(contours, key = cv2.contourArea, reverse = True)[:n]\n",
        "\n",
        "#iterate through contours and draw the convex hull \n",
        "for c in contours: \n",
        "  hull = cv2.convexHull(c)\n",
        "  cv2.drawContours(original_image, [hull], 0, (0,255,0), 2)\n",
        "\n",
        "imshow('Convex Hull', original_image)\n"
      ],
      "metadata": {
        "id": "hTiQGPjz_uCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Matching Contours**\n",
        "#### **cv2.matchShapes(contour template, contour, method, method parameter)**\n",
        "\n",
        "**Output** – match value (lower values means a closer match)\n",
        "\n",
        "- Contour Template – This is our reference contour that we’re trying to find in the new image\n",
        "- Contour – The individual contour we are checking against\n",
        "- Method – Type of contour matching (1, 2, 3)\n",
        "- Method Parameter – leave alone as 0.0 (not fully utilized in python OpenCV)\n"
      ],
      "metadata": {
        "id": "tuoSA80PD5He"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the shape template or reference image\n",
        "template = cv2.imread('/content/images/4star.jpg',0)\n",
        "imshow('Template', template)"
      ],
      "metadata": {
        "id": "AahxWDxT9F1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the target image with the shapes we're trying to match\n",
        "target = cv2.imread('images/shapestomatch.jpg')\n",
        "target_gray = cv2.cvtColor(target,cv2.COLOR_BGR2GRAY)"
      ],
      "metadata": {
        "id": "1jqFqLOpEV5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Threshold both images first before using cv2.findContours\n",
        "ret, thresh1 = cv2.threshold(template, 127, 255, 0)\n",
        "ret, thresh2 = cv2.threshold(target_gray, 127, 255, 0)\n",
        "\n",
        "# Find contours in template\n",
        "contours, hierarchy = cv2.findContours(thresh1, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# We need to sort the contours by area so that we can remove the largest\n",
        "# contour which is the image outline\n",
        "sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
        "\n",
        "# We extract the second largest contour which will be our template contour\n",
        "template_contour = contours[1]\n",
        "\n",
        "# Extract contours from second target image\n",
        "contours, hierarchy = cv2.findContours(thresh2, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "for c in contours:\n",
        "    # Iterate through each contour in the target image and \n",
        "    # use cv2.matchShapes to compare contour shapes\n",
        "    match = cv2.matchShapes(template_contour, c, 3, 0.0)\n",
        "    print(match)\n",
        "    # If the match value is less than 0.15 we\n",
        "    if match < 0.15:\n",
        "        closest_contour = c\n",
        "    else:\n",
        "        closest_contour = [] \n",
        "                \n",
        "cv2.drawContours(target, [closest_contour], -1, (0,255,0), 3)\n",
        "imshow('Output', target)"
      ],
      "metadata": {
        "id": "Ujciq9SoEa2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Line, Circle and blob detection"
      ],
      "metadata": {
        "id": "8gV8TK6RxhBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Line Detection - Using Hough Lines**\n",
        "   \n",
        "The Hough transform takes a binary edge map as input and attempts to locate edges placed as straight lines. The idea of the Hough transform is, that every edge point in the edge map is transformed to all possible lines that could pass through that point.\n",
        "\n",
        "`cv2.HoughLines(binarized/thresholded image, 𝜌 accuracy, 𝜃 accuracy, threshold)`\n",
        "- Threshold here is the minimum vote for it to be considered a line\n"
      ],
      "metadata": {
        "id": "PacXeMM-xobD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"/content/images/soduku.jpg\")\n",
        "imshow('Original', image)\n",
        "\n",
        "#grayscale and canny edges extracted \n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "edges = cv2.Canny(gray, 100, 170, apertureSize = 3)\n",
        "\n",
        "# run hough_lines using a rho accuracy of 1 pixel \n",
        "# theta accuracy of np.pi/180 which is 1 degree \n",
        "# our line threshold is set to 240 (number of points on line)\n",
        "\n",
        "lines = cv2.HoughLines(edges, 1, np.pi/180, 240)\n",
        "\n",
        "#we iterate throuhg each line and onvert it to the format \n",
        "#requied by cv2.lines (i.e., requiring end points)\n",
        "\n",
        "for line in lines:\n",
        "  rho, theta = line[0]\n",
        "  a = np.cos(theta)\n",
        "  b = np.sin(theta)\n",
        "  x0 = a * rho\n",
        "  y0 = b * rho\n",
        "  x1 = int(x0 + 1000*(-b))\n",
        "  y1 = int(y0 + 1000*(a))\n",
        "  x2 = int(x0 - 1000*(-b))\n",
        "  y2 = int(y0 - 1000*(a))\n",
        "  cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "imshow(\"Hough Lines\", image)"
      ],
      "metadata": {
        "id": "Tdjxa9l0Eq9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Probabilistic Hough Lines**\n",
        "A Hough Transform is considered probabilistic if it uses random sampling of the edge points. These algorithms can be divided based on how they map image space to parameter space.\n",
        "\n",
        "```\n",
        "cv2.HoughLinesP(binarized image, 𝜌 accuracy, 𝜃 accuracy, threshold, minimum line length, max line gap)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JbPYqUf13TWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gray scale and canny edges extracted \n",
        "image = cv2.imread(\"/content/images/soduku.jpg\")\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "edges = cv2.Canny(gray, 100, 170, apertureSize = 3)\n",
        "\n",
        "# Again we use the same rho and theta accuracies\n",
        "# However, we specific a minimum vote (pts along line) of 100\n",
        "# and Min line length of 3 pixels and max gap between lines of 25 pixels\n",
        "lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 100, 3, 25)\n",
        "print(lines.shape)\n",
        "\n",
        "for x in range(0, len(lines)):\n",
        "    for x1,y1,x2,y2 in lines[x]:\n",
        "        cv2.line(image,(x1,y1),(x2,y2),(0,255,0),2)\n",
        "\n",
        "imshow('Probabilistic Hough Lines', image)"
      ],
      "metadata": {
        "id": "37bYgqq00EKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Circle Detection - Hough Cirlces**\n",
        "\n",
        "**cv2.HoughCircles**(image, method, dp, MinDist, param1, param2, minRadius, MaxRadius)\n",
        "\n",
        "\n",
        "- Method - currently only cv2.HOUGH_GRADIENT available\n",
        "- dp - Inverse ratio of accumulator resolution\n",
        "- MinDist - the minimum distance between the center of detected circles\n",
        "- param1 - Gradient value used in the edge detection\n",
        "- param2 - Accumulator threshold for the HOUGH_GRADIENT method (lower allows more circles to be detected (false positives))\n",
        "- minRadius - limits the smallest circle to this size (via radius)\n",
        "- MaxRadius - similarly sets the limit for the largest circles\n",
        "\n"
      ],
      "metadata": {
        "id": "g4K17V8Z4yjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/rajeevratan84/ModernComputerVision/main/Circles_Packed_In_Square_11.jpeg"
      ],
      "metadata": {
        "id": "l7TwQPvM4wfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/Circles_Packed_In_Square_11.jpeg')\n",
        "imshow('Circles', image)\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "blur = cv2.medianBlur(gray, 5)\n",
        "\n",
        "circles = cv2.HoughCircles(blur, cv2.HOUGH_GRADIENT, 1.2, 25)\n",
        "\n",
        "cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1.2, 100)\n",
        "\n",
        "circles = np.uint16(np.around(circles))\n",
        "\n",
        "for i in circles[0,:]:\n",
        "    # draw the outer circle\n",
        "    cv2.circle(image,(i[0], i[1]), i[2], (0, 0, 255), 5)\n",
        "    \n",
        "    # draw the center of the circle\n",
        "    cv2.circle(image, (i[0], i[1]), 2, (0, 0, 255), 8)\n",
        "\n",
        "imshow('Detected circles', image)"
      ],
      "metadata": {
        "id": "QAkUt0x75C9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Blob Detection**\n",
        "The function **cv2.drawKeypoints** takes the following arguments:\n",
        "\n",
        "**cv2.drawKeypoints**(input image, keypoints, blank_output_array, color, flags)\n",
        "\n",
        "flags:\n",
        "- cv2.DRAW_MATCHES_FLAGS_DEFAULT\n",
        "- cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
        "- cv2.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG\n",
        "- cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS"
      ],
      "metadata": {
        "id": "7twVJFHM5WkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read image\n",
        "image = cv2.imread(\"images/Sunflowers.jpg\")\n",
        "imshow(\"Original\", image)\n",
        "\n",
        "# Set up the detector with default parameters.\n",
        "detector = cv2.SimpleBlobDetector_create()\n",
        " \n",
        "# Detect blobs.\n",
        "keypoints = detector.detect(image)\n",
        " \n",
        "# Draw detected blobs as red circles.\n",
        "# cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS ensures the size of\n",
        "#the circle corresponds to the size of blob\n",
        "blank = np.zeros((1,1)) \n",
        "blobs = cv2.drawKeypoints(image, keypoints, blank, (0,255,0),\n",
        "                                      cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
        " \n",
        "# Show keypoints\n",
        "imshow(\"Blobs\", blobs)"
      ],
      "metadata": {
        "id": "YMsgQ3y55U5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counting circles, ellipses and Finding Waldo "
      ],
      "metadata": {
        "id": "SKCEl6ls6IwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Counting Circular Blobs**"
      ],
      "metadata": {
        "id": "Ho6WkauH6gCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image\n",
        "image = cv2.imread(\"/content/images/blobs.jpg\", 0)\n",
        "imshow('Original Image',image)"
      ],
      "metadata": {
        "id": "q_vrGS_458p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing detecor using the default parameters \n",
        "detector = cv2.SimpleBlobDetector_create()\n",
        "\n",
        "#detect blobs \n",
        "keypoints = detector.detect(image)\n",
        "\n",
        "#draw blobs on our images as red circles \n",
        "blank = np.zeros((1,1))\n",
        "blobs = cv2.drawKeypoints(image, keypoints, blank, (0,0,255),\n",
        "                          cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "number_of_blobs = len(keypoints)\n",
        "text = \"Total Number of Blobs :\" + str(len(keypoints))\n",
        "cv2.putText(blobs, text, (20, 550), cv2.FONT_HERSHEY_SIMPLEX, 1, (100,0,255),2)\n",
        "\n",
        "#display images with blob kepoints \n",
        "imshow(\"Blobs using default parameters :\", blobs)"
      ],
      "metadata": {
        "id": "3kixIkTc9DcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set our filtering parameters\n",
        "# initialize parameters setting using cv2.SimpleBlobDetector \n",
        "params = cv2.SimpleBlobDetector_Params()\n",
        "\n",
        "#set area filtering parameters \n",
        "params.filterByArea = True \n",
        "params.minArea = 100\n",
        "\n",
        "#set cicularity filtering parameters \n",
        "params.filterByCircularity = True \n",
        "params.minCircularity = 0.9\n",
        "\n",
        "#set convexity filtering parameters \n",
        "params.filterByConvexity = False \n",
        "params.minConvexity = 0.2\n",
        "\n",
        "#set inertia filtering parameters \n",
        "params.filterByInertia = True \n",
        "params.minInertiaRatio = 0.01\n",
        "\n",
        "#create a detector with the parameters \n",
        "detector = cv2.SimpleBlobDetector_create(params)\n",
        "\n",
        "#detect blobs \n",
        "keypoints = detector.detect(image)\n",
        "\n",
        "# Draw blobs on our image as red circles\n",
        "blank = np.zeros((1,1)) \n",
        "blobs = cv2.drawKeypoints(image, keypoints, blank, (0,255,0),\n",
        "                                      cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "number_of_blobs = len(keypoints)\n",
        "text = \"Number of Circular Blobs: \" + str(len(keypoints))\n",
        "cv2.putText(blobs, text, (20, 550), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 100, 255), 2)\n",
        "\n",
        "# Show blobs\n",
        "imshow(\"Filtering Circular Blobs Only\", blobs)"
      ],
      "metadata": {
        "id": "keBsAnLn-C6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Finding Waldo Using Template Matching**\n",
        "\n",
        "#### **Notes on Template Matching**\n",
        "\n",
        "There are a variety of methods to perform template matching, but in this case we are using the correlation coefficient which is specified by the flag **cv2.TM_CCOEFF.**\n",
        "\n",
        "So what exactly is the cv2.matchTemplate function doing?\n",
        "Essentially, this function takes a “sliding window” of our waldo query image and slides it across our puzzle image from left to right and top to bottom, one pixel at a time. Then, for each of these locations, we compute the correlation coefficient to determine how “good” or “bad” the match is. \n",
        "\n",
        "Regions with sufficiently high correlation can be considered “matches” for our waldo template.\n",
        "From there, all we need is a call to cv2.minMaxLoc on Line 22 to find where our “good” matches are.\n",
        "That’s really all there is to template matching!\n",
        "\n",
        "http://docs.opencv.org/2.4/modules/imgproc/doc/object_detection.html      "
      ],
      "metadata": {
        "id": "faouA4QJC7lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = cv2.imread('./images/waldo.jpg')\n",
        "imshow('Template', template)"
      ],
      "metadata": {
        "id": "A7VkvEDDCw1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load input image and convert to grayscale\n",
        "image = cv2.imread('./images/WaldoBeach.jpg')\n",
        "imshow('Where is Waldo?', image)\n",
        "\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Load Template image\n",
        "template = cv2.imread('./images/waldo.jpg',0)\n",
        "\n",
        "result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)\n",
        "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
        "\n",
        "#Create Bounding Box\n",
        "top_left = max_loc\n",
        "bottom_right = (top_left[0] + 50, top_left[1] + 50)\n",
        "cv2.rectangle(image, top_left, bottom_right, (0,0,255), 5)\n",
        "\n",
        "imshow('Where is Waldo?', image)"
      ],
      "metadata": {
        "id": "_pIhYRb5Dil4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding corners"
      ],
      "metadata": {
        "id": "FQ-s2nLzFCNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is a Corner?**\n",
        "\n",
        "A corner is a point whose local neighborhood stands in two dominant and different edge directions. In other words, a corner can be interpreted as the junction of two edges, where an edge is a sudden change in image brightness. Corners are the important features in the image, and they are generally termed as interest points which are invariant to translation, rotation, and illumination.\n",
        "\n",
        "![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/edge.png)"
      ],
      "metadata": {
        "id": "ATeBaqvtF690"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Harris Corner Detection** is an algorithm developed in 1988 for corner detection that works fairly well.\n",
        "\n",
        "\n",
        "**Paper** - http://www.bmva.org/bmvc/1988/avc-88-023.pdf\n",
        "\n",
        "**cv2.cornerHarris**(input image, block size, ksize, k)\n",
        "- Input image - should be grayscale and float32 type.\n",
        "- blockSize - the size of neighborhood considered for corner detection\n",
        "- ksize - aperture parameter of Sobel derivative used.\n",
        "- k - harris detector free parameter in the equation\n",
        "- **Output** – array of corner locations (x,y)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fKeilWPxGO13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image then grayscale\n",
        "image = cv2.imread('./images/chess.JPG')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# The cornerHarris function requires the array datatype to be float32\n",
        "gray = np.float32(gray)\n",
        "\n",
        "harris_corners = cv2.cornerHarris(gray, 3, 3, 0.05)\n",
        "\n",
        "#We use dilation of the corner points to enlarge them\\\n",
        "kernel = np.ones((7,7),np.uint8)\n",
        "harris_corners = cv2.dilate(harris_corners, kernel, iterations = 2)\n",
        "\n",
        "# Threshold for an optimal value, it may vary depending on the image.\n",
        "image[harris_corners > 0.025 * harris_corners.max() ] = [255, 127, 127]\n",
        "\n",
        "imshow('Harris Corners', image)"
      ],
      "metadata": {
        "id": "320OkBkoDs9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cv2.goodFeaturesToTrack**(input image, maxCorners, qualityLevel, minDistance)\n",
        "\n",
        "- Input Image - 8-bit or floating-point 32-bit, single-channel image.\n",
        "- maxCorners – Maximum number of corners to return. If there are more corners than are found, the strongest of them is returned.\n",
        "- qualityLevel – Parameter characterizing the minimal accepted quality of image corners. The parameter value is multiplied by the best corner quality measure (smallest eigenvalue). The corners with the quality measure less than the product are rejected. For example, if the best corner has the quality measure = 1500, and the  qualityLevel=0.01 , then all the corners with the quality - - measure less than 15 are rejected.\n",
        "- minDistance – Minimum possible Euclidean distance between the returned corners.\n"
      ],
      "metadata": {
        "id": "NnxFtG97GzV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('./images/chess.JPG')\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# We specific the top 50 corners\n",
        "corners = cv2.goodFeaturesToTrack(gray, 150, 0.0005, 10)\n",
        "\n",
        "for corner in corners:\n",
        "    x, y = corner[0]\n",
        "    x = int(x)\n",
        "    y = int(y)\n",
        "    cv2.rectangle(img,(x-10,y-10),(x+10,y+10),(0,255,0), 2)\n",
        "    \n",
        "imshow(\"Corners Found\", img)"
      ],
      "metadata": {
        "id": "ajenvuDpGz24"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}